{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-12T09:48:10.545583756Z",
     "start_time": "2023-07-12T09:48:10.417761688Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'base_path': 'C:/SWdacon/minseo/open', # change relative path of data\n",
    "    'train_data': 'zeroto20.csv', # change train data csv name\n",
    "    'test_data': 'test20.csv', # change test data csv name\n",
    "    'seed': 42,\n",
    "    'valid_size': 0.1,\n",
    "    'early_stopping': 15,\n",
    "    'scheduler': True,\n",
    "    'train' : {\n",
    "       'batch_size' : 1,\n",
    "       'num_workers': 0,\n",
    "       'epochs': 100,\n",
    "       'lr': 1e-3,\n",
    "    },\n",
    "    'inference' : {\n",
    "       'batch_size' : 1,\n",
    "       'num_workers': 0,\n",
    "       'threshold': 0.35,\n",
    "    },\n",
    "}\n",
    "custom_transform = {\n",
    "    'train':A.Compose([\n",
    "        A.augmentations.crops.transforms.RandomCrop(224,224,p=1.0),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "    'valid':A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(224,224,p=1.0),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# dice score 계산 함수\n",
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "def calculate_dice_scores(validation_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = validation_df.iloc[:, 3]\n",
    "    gt_mask_rle = validation_df.iloc[:, 4]\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "    return np.mean(dice_scores)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  18    valid:  2\n"
     ]
    }
   ],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "train_df = pd.read_csv(f\"{config['base_path']}/{config['train_data']}\")\n",
    "train, val = train_test_split(train_df, test_size=config['valid_size'], random_state=config['seed'])\n",
    "print(\"train: \", len(train), \"   valid: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_hq_vit_b.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mobile_sam import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry['vit_t'](checkpoint='C:/SAM/MobileSAM/weights/mobile_sam.pt')\n",
    "sam_model = sam_model.to(config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from mobile_sam.utils.transforms import ResizeLongestSide\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_rles = None,infer=False,transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_rles = mask_rles\n",
    "        self.infer = infer\n",
    "        self.preprocess = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transformed_data = defaultdict(dict)\n",
    "        img_path = self.img_paths.iloc[idx]\n",
    "        image = cv2.imread(config['base_path']+img_path[1:])\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=config['device'])\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :].to(config['device'])\n",
    "        input_image = sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        transformed_data['image'] = input_image\n",
    "        transformed_data['input_size']=input_size\n",
    "        transformed_data['original_image_size']= original_image_size\n",
    "\n",
    "        mask_rle = self.mask_rles.iloc[idx]\n",
    "        gt_mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "        if self.transform:\n",
    "            gt_mask = self.transform(image = image, mask=gt_mask)['mask']\n",
    "        return transformed_data, gt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "\n",
    "train_dataset = CustomDataset(img_paths=train['img_path'], mask_rles=train['mask_rle'],transform = custom_transform['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True, num_workers=config['train']['num_workers'])\n",
    "\n",
    "valid_dataset = CustomDataset(img_paths=val['img_path'], mask_rles=val['mask_rle'],transform = custom_transform['valid'])\n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size=config['train']['batch_size'], shuffle=True, num_workers=config['train']['num_workers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {'image': tensor([[[[-0.0458, -0.0458, -0.0458,  ..., -1.3473, -1.3302, -1.3302],\n",
      "          [-0.0458, -0.0458, -0.0458,  ..., -1.3473, -1.3302, -1.3302],\n",
      "          [-0.0629, -0.0629, -0.0629,  ..., -1.3644, -1.3473, -1.3473],\n",
      "          ...,\n",
      "          [-0.8335, -0.8335, -0.8335,  ...,  0.1768,  0.1768,  0.1768],\n",
      "          [-0.8335, -0.8335, -0.8335,  ...,  0.1768,  0.1768,  0.1768],\n",
      "          [-0.8335, -0.8335, -0.8335,  ...,  0.1768,  0.1768,  0.1768]],\n",
      "\n",
      "         [[ 0.0476,  0.0476,  0.0476,  ..., -1.1078, -1.0903, -1.0903],\n",
      "          [ 0.0476,  0.0476,  0.0476,  ..., -1.1078, -1.0903, -1.0903],\n",
      "          [ 0.0301,  0.0301,  0.0301,  ..., -1.1254, -1.1078, -1.1078],\n",
      "          ...,\n",
      "          [-0.6001, -0.6001, -0.6176,  ...,  0.1352,  0.1352,  0.1352],\n",
      "          [-0.6001, -0.6001, -0.6001,  ...,  0.1352,  0.1352,  0.1352],\n",
      "          [-0.6001, -0.6001, -0.6001,  ...,  0.1352,  0.1352,  0.1352]],\n",
      "\n",
      "         [[-0.0092, -0.0092, -0.0092,  ..., -0.7936, -0.7936, -0.7936],\n",
      "          [-0.0092, -0.0092, -0.0092,  ..., -0.7936, -0.7936, -0.7936],\n",
      "          [-0.0267, -0.0267, -0.0267,  ..., -0.8110, -0.8110, -0.8110],\n",
      "          ...,\n",
      "          [-0.5147, -0.5147, -0.5147,  ...,  0.1651,  0.1651,  0.1651],\n",
      "          [-0.5147, -0.5147, -0.5147,  ...,  0.1651,  0.1651,  0.1651],\n",
      "          [-0.5147, -0.5147, -0.5147,  ...,  0.1651,  0.1651,  0.1651]]]]), 'input_size': (1024, 1024), 'original_image_size': (224, 224)})\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[2][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_sam(input_image,input_size, original_image_size, sam):\n",
    "  image_embedding = sam.image_encoder(input_image)\n",
    "  sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "    points=None,\n",
    "    boxes=None,\n",
    "    masks=None,\n",
    "  )\n",
    "  low_res_masks, iou_predictions = sam.mask_decoder(\n",
    "    image_embeddings=image_embedding,\n",
    "    image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "    sparse_prompt_embeddings=sparse_embeddings,\n",
    "    dense_prompt_embeddings=dense_embeddings,\n",
    "    multimask_output=False,\n",
    "  )\n",
    "  upscaled_masks = sam.postprocess_masks(low_res_masks, (input_size[0],input_size[1]), (original_image_size[0],original_image_size[1])).to(config['device'])\n",
    "  binary_mask = normalize(threshold(upscaled_masks, 0, 0))\n",
    "  binary_mask = 1-binary_mask\n",
    "  return binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def validation(config, criterion, valid_loader, val):\n",
    "    sam_model.eval()\n",
    "    valid_loss = 0\n",
    "    result = []\n",
    "    transformed_mask = []\n",
    "    val_df = val.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for transformed_data, gt_mask in tqdm(valid_loader):\n",
    "            input_image = transformed_data['image'][0].to(config['device'])\n",
    "            input_size = transformed_data['input_size']\n",
    "            original_image_size = transformed_data['original_image_size']\n",
    "            \n",
    "            output = predict_sam(input_image,input_size,original_image_size,sam_model)\n",
    "            loss = criterion(output[0], gt_mask.float())\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            mask_rle = rle_encode(output)\n",
    "            if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                result.append(-1)\n",
    "            else:\n",
    "                result.append(mask_rle)\n",
    "        # val_df['valid_mask_rle'] = result\n",
    "        # val_df['transformed_mask_rle'] = list(map(rle_encode, transformed_mask.squeeze().numpy()))\n",
    "        # dice_score = calculate_dice_scores(val_df)\n",
    "\n",
    "    return valid_loss/len(valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def training(config, train_loader, valid_loader, val):\n",
    "    sam_model.to(config['device'])\n",
    "    es_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=config['train']['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=1e-8, verbose=True)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(config['train']['epochs']):\n",
    "        sam_model.train()\n",
    "        epoch_loss = 0\n",
    "        for transformed_data, gt_mask in tqdm(train_loader):\n",
    "            input_image = transformed_data['image'][0].to(config['device'])\n",
    "            input_size = transformed_data['input_size']\n",
    "            original_image_size = transformed_data['original_image_size']\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.requires_grad_(True)\n",
    "            with torch.no_grad():\n",
    "                image_embedding = sam_model.image_encoder(input_image)\n",
    "                sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                    points=None,\n",
    "                    boxes=None,\n",
    "                    masks=None,\n",
    "                )\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "                image_embeddings=image_embedding,\n",
    "                image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "            )\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, (input_size[0],input_size[1]), (original_image_size[0],original_image_size[1])).to(config['device'])\n",
    "            binary_mask = normalize(threshold(upscaled_masks, 0, 0))\n",
    "            output = 1-binary_mask\n",
    "            # output = predict_sam(input_image,input_size,original_image_size,sam_model)\n",
    "            loss = criterion(output[0], gt_mask.float())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "        val_loss = validation(config, criterion, valid_loader, val)\n",
    "        es_count += 1\n",
    "        if min_val_loss > val_loss:\n",
    "            es_count = 0\n",
    "            min_val_loss = val_loss\n",
    "            best_model = sam_model\n",
    "            best_epoch = epoch + 1\n",
    "            print(f\"Epoch [{epoch + 1}] New Minimum Valid Loss!\")\n",
    "\n",
    "        if config['scheduler']:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if es_count == config['early_stopping']:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, Dice Coefficient: {dice_score:6f}, ES Count:, {es_count}')\n",
    "            print(f\"EARLY STOPPING COUNT에 도달했습니다! \\nEARLY STOPPING COUNT: {config['early_stopping']} BEST EPOCH: {best_epoch}\")\n",
    "            print(\"***TRAINING DONE***\")\n",
    "            return best_model\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f} ES Count:, {es_count}')\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"EARLY STOPPING COUNT에 도달하지 않았습니다! \\nEARLY STOPPING COUNT: {config['early_stopping']} BEST EPOCH: {best_epoch}\")\n",
    "    print(\"***TRAINING DONE***\")\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/18 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:53<00:00,  2.98s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] New Minimum Valid Loss!\n",
      "Epoch 1, Train Loss: 1.260007, Valid Loss: 1.276232 ES Count:, 0\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:56<00:00,  3.15s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 1.260007, Valid Loss: 1.276232 ES Count:, 1\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:55<00:00,  3.11s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 1.260007, Valid Loss: 1.276232 ES Count:, 2\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18/18 [00:56<00:00,  3.15s/it]\n",
      "100%|██████████| 2/2 [00:05<00:00,  2.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 1.260007, Valid Loss: 1.276232 ES Count:, 3\n",
      "------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 13/18 [00:42<00:16,  3.27s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[101], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m best_model \u001b[39m=\u001b[39m training(config,train_dataloader, valid_dataloader, val)\n",
      "Cell \u001b[1;32mIn[100], line 22\u001b[0m, in \u001b[0;36mtraining\u001b[1;34m(config, train_loader, valid_loader, val)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 22\u001b[0m     image_embedding \u001b[39m=\u001b[39m sam_model\u001b[39m.\u001b[39;49mimage_encoder(input_image)\n\u001b[0;32m     23\u001b[0m     sparse_embeddings, dense_embeddings \u001b[39m=\u001b[39m sam_model\u001b[39m.\u001b[39mprompt_encoder(\n\u001b[0;32m     24\u001b[0m         points\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m         boxes\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     26\u001b[0m         masks\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     28\u001b[0m low_res_masks, iou_predictions \u001b[39m=\u001b[39m sam_model\u001b[39m.\u001b[39mmask_decoder(\n\u001b[0;32m     29\u001b[0m     image_embeddings\u001b[39m=\u001b[39mimage_embedding,\n\u001b[0;32m     30\u001b[0m     image_pe\u001b[39m=\u001b[39msam_model\u001b[39m.\u001b[39mprompt_encoder\u001b[39m.\u001b[39mget_dense_pe(),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     33\u001b[0m     multimask_output\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     34\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\SAM\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:617\u001b[0m, in \u001b[0;36mTinyViT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 617\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[0;32m    618\u001b[0m     \u001b[39m#x = self.norm_head(x)\u001b[39;00m\n\u001b[0;32m    619\u001b[0m     \u001b[39m#x = self.head(x)\u001b[39;00m\n\u001b[0;32m    620\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mC:\\SAM\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:609\u001b[0m, in \u001b[0;36mTinyViT.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_i, \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers)):\n\u001b[0;32m    608\u001b[0m     layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[i]\n\u001b[1;32m--> 609\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[0;32m    610\u001b[0m B,_,C\u001b[39m=\u001b[39mx\u001b[39m.\u001b[39msize()\n\u001b[0;32m    611\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, \u001b[39m64\u001b[39m, \u001b[39m64\u001b[39m, C)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\SAM\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:441\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    439\u001b[0m         x \u001b[39m=\u001b[39m checkpoint\u001b[39m.\u001b[39mcheckpoint(blk, x)\n\u001b[0;32m    440\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m         x \u001b[39m=\u001b[39m blk(x)\n\u001b[0;32m    442\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    443\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\SAM\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:359\u001b[0m, in \u001b[0;36mTinyViTBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[39m# window partition\u001b[39;00m\n\u001b[0;32m    357\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, nH, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, nW, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, C)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mreshape(\n\u001b[0;32m    358\u001b[0m     B \u001b[39m*\u001b[39m nH \u001b[39m*\u001b[39m nW, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, C)\n\u001b[1;32m--> 359\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattn(x)\n\u001b[0;32m    360\u001b[0m \u001b[39m# window reverse\u001b[39;00m\n\u001b[0;32m    361\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(B, nH, nW, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwindow_size,\n\u001b[0;32m    362\u001b[0m            C)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mreshape(B, pH, pW, C)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mC:\\SAM\\MobileSAM\\mobile_sam\\modeling\\tiny_vit_sam.py:281\u001b[0m, in \u001b[0;36mAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    273\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mpermute(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m3\u001b[39m)\n\u001b[0;32m    275\u001b[0m attn \u001b[39m=\u001b[39m (\n\u001b[0;32m    276\u001b[0m     (q \u001b[39m@\u001b[39m k\u001b[39m.\u001b[39mtranspose(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale\n\u001b[0;32m    277\u001b[0m     \u001b[39m+\u001b[39m\n\u001b[0;32m    278\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_biases[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_bias_idxs]\n\u001b[0;32m    279\u001b[0m      \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mab)\n\u001b[0;32m    280\u001b[0m )\n\u001b[1;32m--> 281\u001b[0m attn \u001b[39m=\u001b[39m attn\u001b[39m.\u001b[39;49msoftmax(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    282\u001b[0m x \u001b[39m=\u001b[39m (attn \u001b[39m@\u001b[39m v)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mreshape(B, N, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdh)\n\u001b[0;32m    283\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_model = training(config,train_dataloader, valid_dataloader, val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
