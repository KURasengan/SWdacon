{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Union\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'base_path': 'open (1)', # change relative path of data\n",
    "    'train_data': 'train.csv', # change train data csv name\n",
    "    'test_data': 'test.csv', # change test data csv name\n",
    "    'seed': 42,\n",
    "    'valid_size': 0.1,\n",
    "    'early_stopping': 100,\n",
    "    'scheduler': True,\n",
    "    'nums_pixel': True,\n",
    "    'train' : {\n",
    "       'batch_size' : 16,\n",
    "       'num_workers': 1,\n",
    "       'epochs': 150,\n",
    "       'lr': 0.001,\n",
    "    },\n",
    "    'inference' : {\n",
    "       'batch_size' : 8,\n",
    "       'num_workers': 1,\n",
    "       'threshold': 0.35,\n",
    "    },\n",
    "}\n",
    "\n",
    "custom_transform = {\n",
    "    'train':A.Compose([\n",
    "        A.augmentations.crops.transforms.RandomCrop(224,224,p=1.0),\n",
    "        A.RandomRotate90(p=0.7),\n",
    "        A.HorizontalFlip(p=0.7),\n",
    "        A.VerticalFlip(p=0.7),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'valid':A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(224,224,p=1.0),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        A.Normalize(),\n",
    "        ToTensorV2()\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs).encode('utf-8')\n",
    "\n",
    "# dice score 계산 함수\n",
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "def calculate_dice_scores(validation_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = validation_df.iloc[:, 3]\n",
    "    gt_mask_rle = validation_df.iloc[:, 4]\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "    dice_scores = [calculate_dice(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)]\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "    return np.mean(dice_scores)\n",
    "\n",
    "def calculate_nums_pixel(validation_df, img_shape=(224, 224)):\n",
    "    '''\n",
    "    Validation의 건물 pixel 수와 Prediction의 건물 pixel 수를 계산합니다.\n",
    "    더 많이 예측하는지, 덜 예측하는지 기준을 잡고 threshold를 조정에 도움이 될 수 있습니다.\n",
    "    '''\n",
    "    eps = 1e-6\n",
    "    batch_temp, count = 0, 0\n",
    "    more_pred,less_pred = 0, 0\n",
    "    pred_mask = validation_df.iloc[:, 2]\n",
    "    gt_mask = validation_df.iloc[:, 3]\n",
    "    for p_mask, t_mask in zip(pred_mask, gt_mask):\n",
    "        if np.sum(rle_decode(t_mask, img_shape)):\n",
    "            count += 1\n",
    "            temp = float(int(np.sum(rle_decode(t_mask, img_shape)) - int(np.sum(rle_decode(p_mask, img_shape)))) / (int(np.sum(rle_decode(t_mask, img_shape))) + eps))\n",
    "            if temp > 0: more_pred+= 1\n",
    "            elif temp < 0: less_pred+= 1\n",
    "            batch_temp += temp\n",
    "    return batch_temp/count, more_pred, less_pred\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "        \n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)       \n",
    "        \n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        \n",
    "        intersection = (inputs * targets).sum()                            \n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)  \n",
    "        \n",
    "        return 1 - dice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "- 1024 * 1024 * 3 이미지 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 사용시 활성화\n",
    "# 드라이브 마운트\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 사용시 활성화\n",
    "\n",
    "# colab_base = '/content/drive/MyDrive/open'# 구글드라이브에서 데이터를 저장한 경로로 바꾸세요.\n",
    "# train_df = pd.read_csv(f\"{colab_base}/{config['train_data']}\")\n",
    "# train_df['img_path'] = colab_base + train_df['img_path'].str[1:]\n",
    "# train, val = train_test_split(train_df, test_size=config['valid_size'], random_state=config['seed'])\n",
    "# print(\"train: \", len(train), \"   valid: \", len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle 사용시 활성화\n",
    "\n",
    "# kaggle_base = /kaggle/input/swdacon # kaggle에서 불러온 데이터의 경로로 바꾸세요.\n",
    "# train_df = pd.read_csv(f\"{kaggle_base}/{config['train_data']}\")\n",
    "# train_df['img_path'] = kaggle_base + train_df['img_path'].str[1:]\n",
    "# train, val = train_test_split(train_df, test_size=config['valid_size'], random_state=config['seed'])\n",
    "# print(\"train: \", len(train), \"   valid: \", len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "train_df = pd.read_csv(f\"{config['base_path']}/{config['train_data']}\")\n",
    "train_df['img_path'] = config['base_path'] + train_df['img_path'].str[1:]\n",
    "train, val = train_test_split(train_df, test_size=config['valid_size'], random_state=config['seed'])\n",
    "print(\"train: \", len(train), \"   valid: \", len(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_rles = None, transform=None, infer=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_rles = mask_rles\n",
    "        self.transform = transform\n",
    "        self.infer = infer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths.iloc[idx]\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.infer:\n",
    "            if self.transform:\n",
    "                image = self.transform(image=image)['image']\n",
    "            return image\n",
    "\n",
    "        mask_rle = self.mask_rles.iloc[idx]\n",
    "        mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "\n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "\n",
    "train_dataset = CustomDataset(img_paths=train['img_path'], mask_rles=train['mask_rle'], transform=custom_transform['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True)\n",
    "\n",
    "valid_dataset = CustomDataset(img_paths=val['img_path'], mask_rles=val['mask_rle'], transform=custom_transform['valid'])\n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size=config['train']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "- 사용하는 모델에 따라 바꾸어야 합니다. 필요한 경우 라이브러리를 install 해야 합니다.\n",
    "- baseline ver2.1의 모델은 현재 최고점인 TransUNet입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vit_pytorch import ViT\n",
    "import torchvision.models as models\n",
    "from einops.layers.torch import Rearrange, Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_encoder = ViT(\n",
    "    image_size=14,\n",
    "    patch_size=1,\n",
    "    dim = 128,\n",
    "    depth=12,\n",
    "    num_classes=2,\n",
    "    heads=8,\n",
    "    mlp_dim=2048,\n",
    ")\n",
    "vit_encoder=vit_encoder.to(config['device'])\n",
    "# depth=12, head=8일때가 현재 최고설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_conv(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "class TransUnet_b5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransUnet_b5, self).__init__()\n",
    "        self.backbone = models.efficientnet_b5(weights='EfficientNet_B5_Weights.DEFAULT')\n",
    "        self.vit_flatten = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=1, p2=1),\n",
    "            nn.LayerNorm(128)\n",
    "        )\n",
    "        self.conv_vit_res = nn.Sequential(\n",
    "            nn.Conv2d(128,64,3,1,1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.res_conv = nn.Sequential(\n",
    "            nn.Conv2d(16,16,3,1,1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.seg_conv = nn.Conv2d(16, 1, 1)\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.dconv_up3 = double_conv(64 + 64, 64)\n",
    "        self.dconv_up2 = double_conv(40 + 64, 32)\n",
    "        self.dconv_up1 = double_conv(32 + 24, 16)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(16, 1, 1)\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.features[0](x) #48,112,112\n",
    "        out1 = self.backbone.features[1](x) #24,112,112\n",
    "        out2 = self.backbone.features[2](out1) #40,56,56\n",
    "        out3 = self.backbone.features[3](out2) #64,28,28\n",
    "        out4 = self.backbone.features[4](out3) #128,14,14\n",
    "\n",
    "        vit_out = self.vit_flatten(out4)\n",
    "        vit_out = vit_encoder.transformer(vit_out)\n",
    "        vit_out = vit_out.reshape(-1,128,14,14)\n",
    "        vit_out = self.conv_vit_res(vit_out) #64,14,14\n",
    "        up3 = self.upsample(vit_out)#64,28,28\n",
    "        up3 = torch.cat([up3, out3], dim=1) #128,28,28\n",
    "        up3 = self.dconv_up3(up3) #64,28,28\n",
    "\n",
    "        up2 = self.upsample(up3) #64,56,56\n",
    "        up2 = torch.cat([up2, out2], dim=1) #40+64,56,56\n",
    "        up2 = self.dconv_up2(up2) #32,56,56\n",
    "\n",
    "        up1 = self.upsample(up2) #32,112,112\n",
    "        up1 = torch.cat([up1,out1],dim=1) #32+24,112,112\n",
    "        up1 = self.dconv_up1(up1) #16,112,112\n",
    "\n",
    "        up0 = self.upsample(up1)#16,224,224\n",
    "        up0 = self.res_conv(up0)\n",
    "        res = self.seg_conv(up0)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransUnet_b5().to(config['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(config, model, criterion, valid_loader, val):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    result = []\n",
    "    transformed_mask = []\n",
    "    val_df = val.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(valid_loader):\n",
    "            if type(transformed_mask) == torch.Tensor:\n",
    "                transformed_mask = torch.cat([transformed_mask, masks])\n",
    "            else:\n",
    "                transformed_mask = masks.clone().detach()\n",
    "            images = images.float().to(config['device'])\n",
    "            masks = masks.float().to(config['device'])\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "            output_masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "            output_masks = np.squeeze(output_masks, axis=1)\n",
    "            output_masks = (output_masks > config['inference']['threshold']).astype(np.uint8)\n",
    "\n",
    "            for i in range(len(images)):\n",
    "                mask_rle = rle_encode(output_masks[i])\n",
    "                if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                    result.append(-1)\n",
    "                else:\n",
    "                    result.append(mask_rle)\n",
    "        val_df['valid_mask_rle'] = result\n",
    "        val_df['transformed_mask_rle'] = list(map(rle_encode, transformed_mask.squeeze().numpy()))\n",
    "        dice_score = calculate_dice_scores(val_df)\n",
    "        if config['nums_pixel']:\n",
    "            mean_error_ratio, more_pred, less_pred = calculate_nums_pixel(val_df)\n",
    "    if config['nums_pixel']:\n",
    "        return valid_loss/len(valid_loader), dice_score, mean_error_ratio, more_pred, less_pred\n",
    "    else:\n",
    "        return valid_loss/len(valid_loader), dice_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(config, model, train_loader, valid_loader, val):\n",
    "    model = model.to(config['device'])\n",
    "    es_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['train']['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, min_lr=1e-8, verbose=True)\n",
    "    print(\"***TRAINING START***\")\n",
    "    # training loop\n",
    "    for epoch in range(config['train']['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for images, masks in tqdm(train_loader):\n",
    "            images = images.float().to(config['device'])\n",
    "            masks = masks.float().to(config['device'])\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        if config['nums_pixel']:\n",
    "            val_loss, dice_score, mean_error_ratio, more_pred, less_pred  = validation(config, model, criterion, valid_loader, val)\n",
    "        else:\n",
    "            val_loss, dice_score = validation(config, model, criterion, valid_loader, val)\n",
    "\n",
    "        es_count += 1\n",
    "        if min_val_loss > val_loss:\n",
    "            es_count = 0\n",
    "            min_val_loss = val_loss\n",
    "            best_model = model\n",
    "            state_dict = model.state_dict()\n",
    "            best_epoch = epoch + 1\n",
    "            print(f\"Epoch [{epoch + 1}] New Minimum Valid Loss!\")\n",
    "            # if epoch > 50: # 몇 epoch 이상부터 저장할지 결정합니다.\n",
    "            #     torch.save(state_dict, f'{config[\"base_path\"]}/current_model.pt') # 비정상적인 종료에 대비해 최고점일 때 모델을 저장하고 싶으면 경로를 지정하고 활성화하십시오.\n",
    "            \n",
    "        if config['scheduler']:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if es_count == config['early_stopping']:\n",
    "            if config['nums_pixel']:\n",
    "                print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, ES Count: {es_count}')\n",
    "                print(f'Dice Coefficient: {dice_score:6f}, (GT - Pred)/GT: {mean_error_ratio:2f}, More Pred : Less Pred = {more_pred} : {less_pred}')\n",
    "            else:\n",
    "                print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, Dice Coefficient: {dice_score:6f}, ES Count: {es_count}')\n",
    "            print(f\"Early Stopping Count에 도달하지 않았습니다! \\nEarly Stopping Count: {config['early_stopping']} Best Epoch: {best_epoch}\")\n",
    "            print(\"***TRAINING DONE***\")\n",
    "            return best_model, state_dict\n",
    "\n",
    "        if config['nums_pixel']:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, ES Count: {es_count}')\n",
    "            print(f'Dice Coefficient: {dice_score:6f}, (GT - Pred)/GT: {mean_error_ratio:2f}, More Pred : Less Pred = {more_pred} : {less_pred}')\n",
    "        else:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, Dice Coefficient: {dice_score:6f}, ES Count: {es_count}')\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"Early Stopping Count에 도달하지 않았습니다! \\nEarly Stopping Count: {config['early_stopping']} Best Epoch: {best_epoch}\")\n",
    "    print(\"***TRAINING DONE***\")\n",
    "    return best_model, state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available() # 학습 전에 GPU 쓰고 있나 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model, best_model_state_dict = training(config, model, train_dataloader, valid_dataloader, val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "- 224 * 224 * 3 이미지 추론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 사용시 활성화\n",
    "\n",
    "# test_df = pd.read_csv(f\"{colab_base}/{config['test_data']}\")\n",
    "# test_df['img_path'] = colab_base + test_df['img_path'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle 사용시 활성화\n",
    "\n",
    "# test_df = pd.read_csv(f\"{kaggle_base}/{config['test_data']}\")\n",
    "# test_df['img_path'] = kaggle_base + test_df['img_path'].str[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "test_df = pd.read_csv(f\"{config['base_path']}/{config['test_data']}\")\n",
    "test_df['img_path'] = config['base_path'] + test_df['img_path'].str[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "\n",
    "test_dataset = CustomDataset(img_paths=test_df['img_path'], transform=custom_transform['test'], infer=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['inference']['batch_size'], shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(config, model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        result = []\n",
    "        for images in tqdm(test_loader):\n",
    "            images = images.float().to(config['device'])\n",
    "            \n",
    "            outputs = model(images)\n",
    "            masks = torch.sigmoid(outputs).cpu().numpy()\n",
    "            masks = np.squeeze(masks, axis=1)\n",
    "            masks = (masks > config['inference']['threshold']).astype(np.uint8)\n",
    "            \n",
    "            for i in range(len(images)):\n",
    "                mask_rle = rle_encode(masks[i])\n",
    "                if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                    result.append(-1)\n",
    "                else:\n",
    "                    result.append(mask_rle)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_result = inference(config, best_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 사용시 활성화\n",
    "\n",
    "# submit = pd.read_csv(f\"{colab_base}/sample_submission.csv\")\n",
    "# submit['mask_rle'] = inference_result\n",
    "\n",
    "# kst = pytz.timezone('Asia/Seoul')\n",
    "# now = datetime.datetime.now(tz=kst)\n",
    "# current_time = now.strftime(\"%y%m%d-%H_%M_%S\")\n",
    "# file_name = f\"{current_time}.csv\"\n",
    "\n",
    "# submit.to_csv(f\"{colab_base}/{file_name}\", index=False)\n",
    "\n",
    "# 모델 저장\n",
    "\n",
    "# model_name = f\"{current_time}.pt\"\n",
    "# torch.save(best_model, f'{colab_base}/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle 사용시 활성화\n",
    "\n",
    "# submit = pd.read_csv(f\"{kaggle_base}/sample_submission.csv\")\n",
    "# submit['mask_rle'] = inference_result\n",
    "\n",
    "# kst = pytz.timezone('Asia/Seoul')\n",
    "# now = datetime.datetime.now(tz=kst)\n",
    "# current_time = now.strftime(\"%y%m%d-%H_%M_%S\")\n",
    "# file_name = f\"{current_time}.csv\"\n",
    "\n",
    "# submit.to_csv(f\"/kaggle/working/{file_name}\", index=False)\n",
    "\n",
    "# 모델 저장\n",
    "\n",
    "# model_name = f\"{current_time}.pt\"\n",
    "# torch.save(best_model, f'/kaggle/working/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "submit = pd.read_csv(f\"{config['base_path']}/sample_submission.csv\")\n",
    "submit['mask_rle'] = inference_result\n",
    "\n",
    "kst = pytz.timezone('Asia/Seoul')\n",
    "now = datetime.datetime.now(tz=kst)\n",
    "current_time = now.strftime(\"%y%m%d-%H_%M_%S\")\n",
    "file_name = f\"{current_time}.csv\"\n",
    "\n",
    "submit.to_csv(f\"{config['base_path']}/{file_name}\", index=False)\n",
    "\n",
    "# 모델 저장\n",
    "\n",
    "model_name = f\"{current_time}.pt\"\n",
    "torch.save(best_model, f\"{config['base_path']}/{model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission Viewer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_viewer(test_csv, submit_csv, img_num, base_path = config['base_path'], is_colab = False):\n",
    "    \"\"\"\n",
    "    white -> 건물 black -> 배경\n",
    "    1. Local에서 사용 시 test_csv, submit_csv, img_num만 입력\n",
    "    2. colab에서 사용 시 아래 사항을 입력\n",
    "    base_path = colab_base\n",
    "    is_colab = True\n",
    "    \"\"\"\n",
    "    mask_rle = submit_csv.iloc[img_num, 1]\n",
    "    image_path = test_csv.iloc[img_num, 1]\n",
    "    if is_colab:\n",
    "        image = cv2.imread(image_path)\n",
    "    else:\n",
    "        image = cv2.imread(base_path + image_path[1:])\n",
    "    mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(1,2,1)\n",
    "    ax1.imshow(image)\n",
    "    ax1.set_title('image')\n",
    "    ax2 = fig.add_subplot(1,2,2)\n",
    "    ax2.imshow(mask,cmap='gray')\n",
    "    ax2.set_title('mask')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colab 사용시 활성화\n",
    "\n",
    "# last_submit = pd.read_csv(f\"{colab_base}/{file_name}\")\n",
    "# submission_viewer(test_df, last_submit, 0, colab_base, is_colab = True)\n",
    "# submission_viewer(test_df, last_submit, 1, colab_base, is_colab = True)\n",
    "# submission_viewer(test_df, last_submit, 2, colab_base, is_colab = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kaggle 사용시 활성화\n",
    "\n",
    "# last_submit = pd.read_csv(f\"/kaggle/working/{file_name}\")\n",
    "# submission_viewer(test_df, last_submit, 0, f'/kaggle/working/{file_name}', is_colab = True)\n",
    "# submission_viewer(test_df, last_submit, 1, f'/kaggle/working/{file_name}', is_colab = True)\n",
    "# submission_viewer(test_df, last_submit, 2, f'/kaggle/working/{file_name}', is_colab = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "last_submit = pd.read_csv(f\"{config['base_path']}/{file_name}\")\n",
    "submission_viewer(test_df, last_submit, 0)\n",
    "submission_viewer(test_df, last_submit, 1)\n",
    "submission_viewer(test_df, last_submit, 2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
