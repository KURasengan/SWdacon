{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-07-12T09:48:10.545583756Z",
     "start_time": "2023-07-12T09:48:10.417761688Z"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libopenh264.so.5: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcv2\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/building/lib/python3.9/site-packages/cv2/__init__.py:181\u001B[0m\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExtra Python code for\u001B[39m\u001B[38;5;124m\"\u001B[39m, submodule, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mis loaded\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    178\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mOpenCV loader: DONE\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 181\u001B[0m \u001B[43mbootstrap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/building/lib/python3.9/site-packages/cv2/__init__.py:153\u001B[0m, in \u001B[0;36mbootstrap\u001B[0;34m()\u001B[0m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m DEBUG: \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRelink everything from native cv2 module to cv2 package\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    151\u001B[0m py_module \u001B[38;5;241m=\u001B[39m sys\u001B[38;5;241m.\u001B[39mmodules\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv2\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 153\u001B[0m native_module \u001B[38;5;241m=\u001B[39m \u001B[43mimportlib\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mimport_module\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcv2\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    155\u001B[0m sys\u001B[38;5;241m.\u001B[39mmodules[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcv2\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m py_module\n\u001B[1;32m    156\u001B[0m \u001B[38;5;28msetattr\u001B[39m(py_module, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_native\u001B[39m\u001B[38;5;124m\"\u001B[39m, native_module)\n",
      "File \u001B[0;32m~/anaconda3/envs/building/lib/python3.9/importlib/__init__.py:127\u001B[0m, in \u001B[0;36mimport_module\u001B[0;34m(name, package)\u001B[0m\n\u001B[1;32m    125\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[1;32m    126\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m--> 127\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_bootstrap\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_gcd_import\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m[\u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpackage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mImportError\u001B[0m: libopenh264.so.5: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "import os\n",
    "from typing import List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.functional import threshold, normalize\n",
    "\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config = {\n",
    "    'device': torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "    'base_path': '/home/jo/Desktop/swdacon/open20', # change relative path of data\n",
    "    'train_data': 'zeroto20.csv', # change train data csv name\n",
    "    'test_data': 'test20.csv', # change test data csv name\n",
    "    'seed': 42,\n",
    "    'valid_size': 0.1,\n",
    "    'early_stopping': 15,\n",
    "    'scheduler': True,\n",
    "    'train' : {\n",
    "       'batch_size' : 1,\n",
    "       'num_workers': 8,\n",
    "       'epochs': 100,\n",
    "       'lr': 0.0005,\n",
    "    },\n",
    "    'inference' : {\n",
    "       'batch_size' : 8,\n",
    "       'num_workers': 8,\n",
    "       'threshold': 0.35,\n",
    "    },\n",
    "}\n",
    "custom_transform = {\n",
    "    'train':A.Compose([\n",
    "        A.augmentations.crops.transforms.RandomCrop(224,224,p=1.0),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "    'valid':A.Compose([\n",
    "        A.augmentations.crops.transforms.CenterCrop(224,224,p=1.0),\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "    'test': A.Compose([\n",
    "        # A.Normalize(),\n",
    "        # ToTensorV2()\n",
    "    ]),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "config['device']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 시드 고정 함수\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "# RLE 디코딩 함수\n",
    "def rle_decode(mask_rle: Union[str, int], shape=(224, 224)) -> np.array:\n",
    "    '''\n",
    "    mask_rle: run-length as string formatted (start length)\n",
    "    shape: (height,width) of array to return\n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    if mask_rle == -1:\n",
    "        return np.zeros(shape)\n",
    "\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape)\n",
    "\n",
    "# RLE 인코딩 함수\n",
    "def rle_encode(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "# dice score 계산 함수\n",
    "def dice_score(prediction: np.array, ground_truth: np.array, smooth=1e-7) -> float:\n",
    "    '''\n",
    "    Calculate Dice Score between two binary masks.\n",
    "    '''\n",
    "    intersection = np.sum(prediction * ground_truth)\n",
    "    return (2.0 * intersection + smooth) / (np.sum(prediction) + np.sum(ground_truth) + smooth)\n",
    "\n",
    "def calculate_dice_scores(validation_df, img_shape=(224, 224)) -> List[float]:\n",
    "    '''\n",
    "    Calculate Dice scores for a dataset.\n",
    "    '''\n",
    "    # Extract the mask_rle columns\n",
    "    pred_mask_rle = validation_df.iloc[:, 3]\n",
    "    gt_mask_rle = validation_df.iloc[:, 4]\n",
    "\n",
    "    def calculate_dice(pred_rle, gt_rle):\n",
    "        pred_mask = rle_decode(pred_rle, img_shape)\n",
    "        gt_mask = rle_decode(gt_rle, img_shape)\n",
    "        if np.sum(gt_mask) > 0 or np.sum(pred_mask) > 0:\n",
    "            return dice_score(pred_mask, gt_mask)\n",
    "        else:\n",
    "            return None  # No valid masks found, return None\n",
    "    dice_scores = Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_dice)(pred_rle, gt_rle) for pred_rle, gt_rle in zip(pred_mask_rle, gt_mask_rle)\n",
    "    )\n",
    "    dice_scores = [score for score in dice_scores if score is not None]  # Exclude None values\n",
    "    return np.mean(dice_scores)\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, weight=None, size_average=True):\n",
    "        super(DiceLoss, self).__init__()\n",
    "\n",
    "    def forward(self, inputs, targets, smooth=1):\n",
    "\n",
    "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
    "        inputs = F.sigmoid(inputs)\n",
    "\n",
    "        #flatten label and prediction tensors\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2.*intersection + smooth)/(inputs.sum() + targets.sum() + smooth)\n",
    "\n",
    "        return 1 - dice"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# 로컬 사용시 활성화\n",
    "\n",
    "train_df = pd.read_csv(f\"{config['base_path']}/{config['train_data']}\")\n",
    "train, val = train_test_split(train_df, test_size=config['valid_size'], random_state=config['seed'])\n",
    "print(\"train: \", len(train), \"   valid: \", len(val))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model_type = 'vit_b'\n",
    "checkpoint = 'sam_hq_vit_b.pth'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "sam_model = sam_model_registry[model_type](checkpoint='/home/jo/Desktop/swdacon/sam-hq/checkpoints/'+checkpoint)\n",
    "sam_model = sam_model.to(config['device'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### dataloader"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "# from mobile_sam.utils.transforms import ResizeLongestSide\n",
    "from collections import defaultdict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_paths, mask_rles = None,infer=False,transform=None):\n",
    "        self.img_paths = img_paths\n",
    "        self.mask_rles = mask_rles\n",
    "        self.infer = infer\n",
    "        self.preprocess = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transformed_data = defaultdict(dict)\n",
    "        img_path = self.img_paths.iloc[idx]\n",
    "        image = cv2.imread(config['base_path']+img_path[1:])\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image=image)['image']\n",
    "        transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "        input_image = transform.apply_image(image)\n",
    "        input_image_torch = torch.as_tensor(input_image, device=config['device'])\n",
    "        transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :].to(config['device'])\n",
    "        input_image = sam_model.preprocess(transformed_image)\n",
    "        original_image_size = image.shape[:2]\n",
    "        input_size = tuple(transformed_image.shape[-2:])\n",
    "        transformed_data['image'] = input_image\n",
    "        transformed_data['input_size']=input_size\n",
    "        transformed_data['original_image_size']= original_image_size\n",
    "\n",
    "        mask_rle = self.mask_rles.iloc[idx]\n",
    "        gt_mask = rle_decode(mask_rle, (image.shape[0], image.shape[1]))\n",
    "        if self.transform:\n",
    "            gt_mask = self.transform(image = image, mask=gt_mask)['mask']\n",
    "        return transformed_data, gt_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fix_seed(config['seed'])\n",
    "\n",
    "train_dataset = CustomDataset(img_paths=train['img_path'], mask_rles=train['mask_rle'],transform = custom_transform['train'])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['train']['batch_size'], shuffle=True, num_workers=config['train']['num_workers'])\n",
    "\n",
    "valid_dataset = CustomDataset(img_paths=val['img_path'], mask_rles=val['mask_rle'],transform = custom_transform['valid'])\n",
    "valid_dataloader = DataLoader(valid_dataset , batch_size=config['train']['batch_size'], shuffle=True, num_workers=config['train']['num_workers'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(train_dataset[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predict_sam(input_image,input_size, original_image_size, sam):\n",
    "    image_embedding = sam.image_encoder(input_image)\n",
    "    sparse_embeddings, dense_embeddings = sam.prompt_encoder(\n",
    "      points=None,\n",
    "      boxes=None,\n",
    "      masks=None,\n",
    "    )\n",
    "    low_res_masks, iou_predictions = sam.mask_decoder(\n",
    "      image_embeddings=image_embedding,\n",
    "      image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "      sparse_prompt_embeddings=sparse_embeddings,\n",
    "      dense_prompt_embeddings=dense_embeddings,\n",
    "      multimask_output=False,\n",
    "    )\n",
    "    upscaled_masks = sam.postprocess_masks(low_res_masks, input_size, original_image_size).to(config['device'])\n",
    "    binary_mask = normalize(threshold(upscaled_masks, 0.0, 0))\n",
    "    return binary_mask"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def validation(config, model, criterion, valid_loader, val):\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    result = []\n",
    "    transformed_mask = []\n",
    "    val_df = val.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for transformed_data, gt_masks in tqdm(valid_loader):\n",
    "            if type(transformed_mask) == torch.Tensor:\n",
    "                transformed_mask = torch.cat([transformed_mask, gt_masks])\n",
    "            else:\n",
    "                transformed_mask = gt_masks.clone().detach()\n",
    "            output_masks=[]\n",
    "            for index in range(len(gt_masks)):\n",
    "                input_image = transformed_data['image'][index].to(config['device'])\n",
    "                input_size = transformed_data['image'][index].to(config['device'])\n",
    "                original_image_size = transformed_data['image'][index].to(config['device'])\n",
    "                gt_mask = gt_masks[index]\n",
    "                output = predict_sam(input_image,input_size,original_image_size,model)\n",
    "                loss = criterion(output, gt_mask.unsqueeze(1))\n",
    "                valid_loss += loss.item()\n",
    "                output_masks.append(output)\n",
    "\n",
    "            for i in range(len(gt_masks)):\n",
    "                mask_rle = rle_encode(output_masks[i])\n",
    "                if mask_rle == '': # 예측된 건물 픽셀이 아예 없는 경우 -1\n",
    "                    result.append(-1)\n",
    "                else:\n",
    "                    result.append(mask_rle)\n",
    "        val_df['valid_mask_rle'] = result\n",
    "        val_df['transformed_mask_rle'] = list(map(rle_encode, transformed_mask.squeeze().numpy()))\n",
    "        dice_score = calculate_dice_scores(val_df)\n",
    "\n",
    "    return valid_loss/len(valid_loader), dice_score"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def training(config, model, train_loader, valid_loader, val):\n",
    "    model = model.to(config['device'])\n",
    "    es_count = 0\n",
    "    min_val_loss = float('inf')\n",
    "    best_model = None\n",
    "\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['train']['lr'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=4, min_lr=1e-8, verbose=True)\n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(config['train']['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for transformed_data, gt_masks in tqdm(train_loader):\n",
    "            for index in range(len(gt_masks)):\n",
    "                input_image = transformed_data['image'][index].to(config['device'])\n",
    "                input_size = transformed_data['image'][index].to(config['device'])\n",
    "                original_image_size = transformed_data['image'][index].to(config['device'])\n",
    "                gt_mask = gt_masks[index]\n",
    "                output = predict_sam(input_image,input_size,original_image_size,model)\n",
    "                loss = criterion(output, gt_mask.unsqueeze(1))\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "        val_loss, dice_score = validation(config, model, criterion, valid_loader, val)\n",
    "        es_count += 1\n",
    "        if min_val_loss > val_loss:\n",
    "            es_count = 0\n",
    "            min_val_loss = val_loss\n",
    "            best_model = model\n",
    "            best_epoch = epoch + 1\n",
    "            print(f\"Epoch [{epoch + 1}] New Minimum Valid Loss!\")\n",
    "\n",
    "        if config['scheduler']:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if es_count == config['early_stopping']:\n",
    "            print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, Dice Coefficient: {dice_score:6f}, ES Count:, {es_count}')\n",
    "            print(f\"EARLY STOPPING COUNT에 도달했습니다! \\nEARLY STOPPING COUNT: {config['early_stopping']} BEST EPOCH: {best_epoch}\")\n",
    "            print(\"***TRAINING DONE***\")\n",
    "            return best_model\n",
    "\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {(epoch_loss/len(train_loader)):6f}, Valid Loss: {val_loss:6f}, Dice Coefficient: {dice_score:6f} ES Count:, {es_count}')\n",
    "        print(\"------------------------------------------------------------------------------------\")\n",
    "\n",
    "    print(f\"EARLY STOPPING COUNT에 도달하지 않았습니다! \\nEARLY STOPPING COUNT: {config['early_stopping']} BEST EPOCH: {best_epoch}\")\n",
    "    print(\"***TRAINING DONE***\")\n",
    "    return best_model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "torch.multiprocessing.set_start_method('spawn')\n",
    "best_model = training(config, sam_model,train_dataloader, valid_dataloader, val)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
